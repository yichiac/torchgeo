[rank: 0] Seed set to 0
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/projects/bcnh/spack/var/spack/environments/dali/.spack-env/view/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name          | Type             | Params
---------------------------------------------------
0 | criterion     | CrossEntropyLoss | 0     
1 | train_metrics | MetricCollection | 0     
2 | val_metrics   | MetricCollection | 0     
3 | test_metrics  | MetricCollection | 0     
4 | model         | Unet             | 32.6 M
---------------------------------------------------
32.6 M    Trainable params
0         Non-trainable params
32.6 M    Total params
130.287   Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
Converting CDL CRS from EPSG:5070 to EPSG:3857
Converting CDL res from 30.0 to 10
Sanity Checking: |          | 0/? [00:00<?, ?it/s]/projects/bcnh/spack/var/spack/environments/dali/.spack-env/view/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:02<00:02,  0.36it/s]Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:04<00:00,  0.48it/s]                                                                           /projects/bcnh/spack/var/spack/environments/dali/.spack-env/view/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training:   0%|          | 0/5119772 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/5119772 [00:00<?, ?it/s] Epoch 0:   0%|          | 1/5119772 [00:11<16553:08:54,  0.09it/s]Epoch 0:   0%|          | 1/5119772 [00:11<16553:33:31,  0.09it/s, v_num=1]Epoch 0:   0%|          | 2/5119772 [00:21<15627:31:49,  0.09it/s, v_num=1]Epoch 0:   0%|          | 2/5119772 [00:21<15627:41:55,  0.09it/s, v_num=1]Epoch 0:   0%|          | 3/5119772 [00:30<14676:17:13,  0.10it/s, v_num=1]Epoch 0:   0%|          | 3/5119772 [00:30<14676:24:05,  0.10it/s, v_num=1]Epoch 0:   0%|          | 4/5119772 [00:43<15434:46:14,  0.09it/s, v_num=1]Epoch 0:   0%|          | 4/5119772 [00:43<15434:52:08,  0.09it/s, v_num=1]Epoch 0:   0%|          | 5/5119772 [00:53<15181:13:52,  0.09it/s, v_num=1]Epoch 0:   0%|          | 5/5119772 [00:53<15181:17:59,  0.09it/s, v_num=1]Epoch 0:   0%|          | 6/5119772 [01:02<14913:07:38,  0.10it/s, v_num=1]Epoch 0:   0%|          | 6/5119772 [01:02<14913:11:01,  0.10it/s, v_num=1]Epoch 0:   0%|          | 7/5119772 [01:11<14521:52:27,  0.10it/s, v_num=1]Epoch 0:   0%|          | 7/5119772 [01:11<14521:55:20,  0.10it/s, v_num=1]Epoch 0:   0%|          | 8/5119772 [01:19<14054:08:06,  0.10it/s, v_num=1]Epoch 0:   0%|          | 8/5119772 [01:19<14054:10:31,  0.10it/s, v_num=1]Epoch 0:   0%|          | 9/5119772 [01:33<14712:57:18,  0.10it/s, v_num=1]Epoch 0:   0%|          | 9/5119772 [01:33<14712:59:35,  0.10it/s, v_num=1]Epoch 0:   0%|          | 10/5119772 [01:47<15240:51:44,  0.09it/s, v_num=1]Epoch 0:   0%|          | 10/5119772 [01:47<15240:54:05,  0.09it/s, v_num=1]Epoch 0:   0%|          | 11/5119772 [01:55<14937:48:37,  0.10it/s, v_num=1]Epoch 0:   0%|          | 11/5119772 [01:55<14937:50:26,  0.10it/s, v_num=1]Epoch 0:   0%|          | 12/5119772 [02:04<14704:15:41,  0.10it/s, v_num=1]Epoch 0:   0%|          | 12/5119772 [02:04<14704:17:21,  0.10it/s, v_num=1]Epoch 0:   0%|          | 13/5119772 [02:14<14759:18:30,  0.10it/s, v_num=1]Epoch 0:   0%|          | 13/5119772 [02:14<14759:20:01,  0.10it/s, v_num=1]Epoch 0:   0%|          | 14/5119772 [02:25<14787:42:43,  0.10it/s, v_num=1]Epoch 0:   0%|          | 14/5119772 [02:25<14787:50:18,  0.10it/s, v_num=1]Epoch 0:   0%|          | 15/5119772 [02:37<14929:05:25,  0.10it/s, v_num=1]Epoch 0:   0%|          | 15/5119772 [02:37<14929:11:35,  0.10it/s, v_num=1]Epoch 0:   0%|          | 16/5119772 [02:42<14422:47:47,  0.10it/s, v_num=1]Epoch 0:   0%|          | 16/5119772 [02:42<14422:54:25,  0.10it/s, v_num=1]Epoch 0:   0%|          | 17/5119772 [02:46<13894:55:53,  0.10it/s, v_num=1]Epoch 0:   0%|          | 17/5119772 [02:46<13895:02:04,  0.10it/s, v_num=1]Epoch 0:   0%|          | 18/5119772 [02:50<13468:36:58,  0.11it/s, v_num=1]Epoch 0:   0%|          | 18/5119772 [02:50<13468:54:22,  0.11it/s, v_num=1]Epoch 0:   0%|          | 19/5119772 [03:04<13789:10:11,  0.10it/s, v_num=1]Epoch 0:   0%|          | 19/5119772 [03:04<13789:16:04,  0.10it/s, v_num=1]Epoch 0:   0%|          | 20/5119772 [03:10<13567:08:13,  0.10it/s, v_num=1]Epoch 0:   0%|          | 20/5119772 [03:10<13567:09:28,  0.10it/s, v_num=1]Epoch 0:   0%|          | 21/5119772 [03:21<13643:48:00,  0.10it/s, v_num=1]Epoch 0:   0%|          | 21/5119772 [03:21<13643:48:58,  0.10it/s, v_num=1]Epoch 0:   0%|          | 22/5119772 [03:32<13737:42:52,  0.10it/s, v_num=1]Epoch 0:   0%|          | 22/5119772 [03:32<13737:43:59,  0.10it/s, v_num=1]Epoch 0:   0%|          | 23/5119772 [03:39<13586:27:17,  0.10it/s, v_num=1]Epoch 0:   0%|          | 23/5119772 [03:39<13586:28:21,  0.10it/s, v_num=1]Epoch 0:   0%|          | 24/5119772 [03:51<13705:39:24,  0.10it/s, v_num=1]Epoch 0:   0%|          | 24/5119772 [03:51<13705:40:21,  0.10it/s, v_num=1]Epoch 0:   0%|          | 25/5119772 [03:58<13584:30:53,  0.10it/s, v_num=1]Epoch 0:   0%|          | 25/5119772 [03:58<13584:31:43,  0.10it/s, v_num=1]Epoch 0:   0%|          | 26/5119772 [04:11<13734:37:36,  0.10it/s, v_num=1]Epoch 0:   0%|          | 26/5119772 [04:11<13734:38:34,  0.10it/s, v_num=1]Epoch 0:   0%|          | 27/5119772 [04:24<13916:24:47,  0.10it/s, v_num=1]Epoch 0:   0%|          | 27/5119772 [04:24<13916:25:31,  0.10it/s, v_num=1]Epoch 0:   0%|          | 28/5119772 [04:27<13587:57:33,  0.10it/s, v_num=1]Epoch 0:   0%|          | 28/5119772 [04:27<13587:58:25,  0.10it/s, v_num=1]Epoch 0:   0%|          | 29/5119772 [04:33<13396:45:26,  0.11it/s, v_num=1]Epoch 0:   0%|          | 29/5119772 [04:33<13396:46:06,  0.11it/s, v_num=1]Epoch 0:   0%|          | 30/5119772 [04:35<13040:39:38,  0.11it/s, v_num=1]Epoch 0:   0%|          | 30/5119772 [04:35<13040:40:17,  0.11it/s, v_num=1]Epoch 0:   0%|          | 31/5119772 [04:47<13209:53:00,  0.11it/s, v_num=1]Epoch 0:   0%|          | 31/5119772 [04:47<13209:53:50,  0.11it/s, v_num=1]slurmstepd: error: *** JOB 3116454 ON gpua023 CANCELLED AT 2024-03-04T00:42:15 ***
